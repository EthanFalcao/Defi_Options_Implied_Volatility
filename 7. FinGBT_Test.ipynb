{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0+cu121)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/138.0 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/138.0 kB ? eta -:--:--\n",
      "     ---------------- -------------------- 61.4/138.0 kB 812.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 138.0/138.0 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting peft\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ethan vaz falcao\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.4.28-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.9/41.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil in c:\\users\\ethan vaz falcao\\appdata\\roaming\\python\\python312\\site-packages (from peft) (5.9.8)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-0.30.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ethan vaz falcao\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ethan vaz falcao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/9.0 MB 9.6 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.3/9.0 MB 4.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/9.0 MB 3.7 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/9.0 MB 3.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/9.0 MB 3.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.9/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.2/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.4/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.5/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.7/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.9/9.0 MB 3.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.1/9.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.3/9.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.5/9.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.7/9.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.0/9.0 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.2/9.0 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.5/9.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.8/9.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.1/9.0 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.2/9.0 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.5/9.0 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.8/9.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.1/9.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.4/9.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.8/9.0 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.1/9.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.5/9.0 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.8/9.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.1/9.0 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/9.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.9/9.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.0/9.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.3/9.0 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/9.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 199.1/199.1 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.4/302.4 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.2 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 317.4/401.2 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 401.2/401.2 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 138.7/138.7 kB 8.0 MB/s eta 0:00:00\n",
      "Downloading regex-2024.4.28-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.5/268.5 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp312-none-win_amd64.whl (289 kB)\n",
      "   ---------------------------------------- 0.0/289.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 289.4/289.4 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.4/2.2 MB 13.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.2 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 10.1 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, regex, pyyaml, huggingface-hub, tokenizers, accelerate, transformers, peft\n",
      "Successfully installed accelerate-0.30.0 huggingface-hub-0.23.0 peft-0.10.0 pyyaml-6.0.1 regex-2024.4.28 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.40.2\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ethan Vaz Falcao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Ethan Vaz Falcao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'meta-llama/Llama-2-7b-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinGPT/fingpt-mt_llama2-7b_lora\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the tokenizer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load the base model with 8-bit precision to save memory\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     15\u001b[0m     base_model, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ethan Vaz Falcao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2067\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2068\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2069\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2070\u001b[0m     )\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m-> 2073\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2074\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2076\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2077\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2078\u001b[0m     )\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'meta-llama/Llama-2-7b-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizerFast\n",
    "from peft import PeftModel\n",
    "\n",
    "# Base Llama 2 model\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# FinGPT LoRA model (replace with the actual FinGPT model)\n",
    "peft_model = \"FinGPT/fingpt-mt_llama2-7b_lora\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "# Load the base model with 8-bit precision to save memory\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model, trust_remote_code=True, device_map=\"auto\", load_in_8bit=True\n",
    ")\n",
    "\n",
    "# Load the PEFT model and combine with the base model\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model = model.eval()\n",
    "\n",
    "# Set pad token to the EOS token (often used for text generation tasks)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate financial text predictions\n",
    "def generate_prediction(prompt):\n",
    "    # Tokenize the prompt\n",
    "    tokens = tokenizer(prompt, return_tensors='pt', padding=True, max_length=2048)\n",
    "    # Generate the prediction\n",
    "    res = model.generate(**tokens, max_length=1024)\n",
    "    # Decode the results to readable text\n",
    "    res_sentences = [tokenizer.decode(i) for i in res]\n",
    "    return res_sentences\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"Provide a summary of the latest financial trends for Q1 2024.\"\n",
    "predictions = generate_prediction(prompt)\n",
    "\n",
    "# Display the predictions\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    print(f\"Prediction {idx + 1}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 73424242434\n",
    "\n",
    "\n",
    "temp= []\n",
    "for n  in range (1,n+1):\n",
    "    if n %3 ==0:\n",
    "        temp.append(n)\n",
    "        pass\n",
    "    elif n %5==0:\n",
    "        temp.append(n)\n",
    "        pass\n",
    "    elif n %7==0 :\n",
    "        temp.append(n)\n",
    "        pass       \n",
    "\n",
    "\n",
    "print(sum(temp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
